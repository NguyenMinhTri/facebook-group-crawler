{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#happiness\n",
    "import labMTsimple.storyLab as lmt\n",
    "lang = 'spanish'\n",
    "labMT,labMTvector,labMTwordList = lmt.emotionFileReader(stopval=0.0,lang=lang,returnVector=True)\n",
    "rightV,rightFval = lmt.emotion(\" \".join(left_words),labMT,shift=True,happsList=labMTvector)\n",
    "leftV,leftFval = lmt.emotion(\" \".join(right_words),labMT,shift=True,happsList=labMTvector)\n",
    "\n",
    "rightStoppedVec = lmt.stopper(rightFval,labMTvector,labMTwordList,stopVal=1.0)\n",
    "leftStoppedVec = lmt.stopper(leftFval,labMTvector,labMTwordList,stopVal=1.0)\n",
    "\n",
    "np.sum(np.array(labMTvector)*np.array(rightStoppedVec))/np.sum(rightStoppedVec), \\\n",
    "np.sum(np.array(labMTvector)*np.array(leftStoppedVec))/np.sum(leftStoppedVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "import lda\n",
    "import pylab as plt\n",
    "from collections import Counter\n",
    "import bisect\n",
    "import re\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "#Delete punctuation\n",
    "def remove_punctuation(string_to_remove):\n",
    "    transtable = {ord(c): None for c in string.punctuation}\n",
    "    return string_to_remove.translate(transtable).lower()\n",
    "\n",
    "def remove_stop_words(text,cached_stop):\n",
    "    text = ' '.join([word for word in text.lower().split() if word not in cached_stop])\n",
    "    return text\n",
    "\n",
    "#Remove endings\n",
    "def stem_string(string_to_stem,language=\"english\"):\n",
    "    from nltk.stem.snowball import SnowballStemmer\n",
    "    stemmer = SnowballStemmer(language)\n",
    "    return \" \".join([stemmer.stem(word) for word in string_to_stem.split(\" \")])\n",
    "\n",
    "def bi_contains(lst, item):\n",
    "    \"\"\" efficient `item in lst` for sorted lists \"\"\"\n",
    "    pos = bisect.bisect_left(lst, item)\n",
    "    return [((item <= lst[-1]) and (lst[pos] == item)),pos]\n",
    "\n",
    "def create_dictionary(lang,cached_stop):\n",
    "    \"\"\"\n",
    "    top 5000 words, using labMTsimple\n",
    "    \"\"\"\n",
    "    from labMTsimple.storyLab import emotionFileReader, emotion, stopper, emotionV\n",
    "    labMT,labMTvector,labMTwordList = emotionFileReader(stopval=0.0,lang=lang,returnVector=True)\n",
    "    vocab = sorted(list(set(labMTwordList) - set(cached_stop)))\n",
    "    pickle.dump(vocab,open('./data/vocab.txt',\"wb+\"))\n",
    "    \n",
    "def create_corpus(all_articles,language=\"english\"):\n",
    "    \"\"\"\n",
    "    create the corpus (a numpy array (kind of a list of lists) \n",
    "    with the number of times each word appear among all articles)\n",
    "    \"\"\"\n",
    "    vocab = pickle.load(open('./data/vocab.txt',\"rb+\"))\n",
    "    len_vocab = len(vocab)\n",
    "    print(len_vocab)\n",
    "    print(\"Number of articles: {0}\".format(len(all_articles)))\n",
    "    allMessages = np.zeros((len(all_articles),len_vocab))\n",
    "    j = -1\n",
    "    for mess in all_articles:\n",
    "        mess = remove_stop_words(remove_punctuation(mess),cached_stop)\n",
    "        j += 1\n",
    "        # Kind-of efficient way to count words (better to use a dictionary)\n",
    "        file = re.split(' |\\n',mess)\n",
    "        c = Counter(file)\n",
    "        del c['']\n",
    "        values = np.zeros(len_vocab)\n",
    "        for word in c:\n",
    "            pos = bi_contains(vocab, word)\n",
    "            if pos[0]:\n",
    "                values[pos[1]] = c[word]\n",
    "        allMessages[j,:] += values\n",
    "        \n",
    "    allMessages = np.array(allMessages,dtype=int)\n",
    "    pickle.dump(allMessages, open('./data/corpus.dat',\"wb+\"))\n",
    "\n",
    "def lda_run(allMessages,vocab,topicNum=10,n_iter=1000,print_all=False):\n",
    "    \"\"\"\n",
    "    run lda with allMessages and print topics\n",
    "    \"\"\"\n",
    "    print(topicNum)\n",
    "    model = lda.LDA(n_topics=topicNum, n_iter=n_iter, random_state=1)\n",
    "    \n",
    "    model.fit(allMessages)\n",
    "    topic_word = model.topic_word_\n",
    "    if print_all: print(model.loglikelihood())\n",
    "    np.savetxt(\"./data/ldaTopics\"+str(topicNum)+\".dat\",np.asarray(topic_word))\n",
    "    for i, topic_dist in enumerate(topic_word):\n",
    "        topic_words = np.array(vocab)[np.argsort(topic_dist)][:]#-[:n_top_words:-1]\n",
    "        freq_words = np.array(topic_dist)[np.argsort(topic_dist)][:]#-n_top_words:-1]\n",
    "\n",
    "        if print_all: print('Topic {}: {}'.format(i, ' '.join(topic_words[::-1][:5])))\n",
    "        if print_all: print('Topic {}: {}'.format(i, ' '.join([str(_) for _ in freq_words[::-1][:5]])))\n",
    "\n",
    "\n",
    "    doc_topic = model.doc_topic_\n",
    "\n",
    "    allDisSorted = []\n",
    "    for i in range(allMessages.shape[0]):\n",
    "        allDisSorted.append(doc_topic[i])\n",
    "\n",
    "    allDisSorted = np.asarray(allDisSorted)\n",
    "    np.savetxt(\"./data/allDistComb1Day\"+str(topicNum)+\".dat\",allDisSorted)\n",
    "    return model.loglikelihood()\n",
    "    \n",
    "\n",
    "def plotNumTopics(x,y,ax2,loglikelihood=False):\n",
    "    \"\"\"\n",
    "    plot points and the smooth line. if KL divergence then divides the results by comb(x,2) to get the mean \n",
    "    \"\"\"\n",
    "    from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "    import pylab as plt\n",
    "    from scipy.misc import comb\n",
    "    x = np.asarray(x)\n",
    "    if loglikelihood:\n",
    "        y = np.asarray(y)\n",
    "    else:\n",
    "        y = np.asarray(y)/comb(x,2)\n",
    "            \n",
    "    a = lowess(y,x,frac = 0.3)\n",
    "    ax2.plot(x,y,'.',linewidth=2,markersize=7,color='orange')\n",
    "\n",
    "    ax2.plot(x,a[:,1],color='orange',linewidth=2,label='Distance')\n",
    "    ax2.set_xlabel('Number of Topics',fontsize=12)\n",
    "    ax2.set_ylabel('Average distance between topic',fontsize=12)\n",
    "\n",
    "\n",
    "\n",
    "def find_distances(tit=\"\",retTopics = False):\n",
    "    \"\"\"\n",
    "    Finds the distance between topics using:\n",
    "    (1) the frequency at which each topic appears every day (topics that always appear in the same dates are probably related) \n",
    "    (2) the frequency of words appearing in the topics (topics with the same words are probably related)\n",
    "    \"\"\"\n",
    "    from scipy.stats import pearsonr,spearmanr,kendalltau,entropy\n",
    "    import pylab as plt\n",
    "    from scipy.spatial.distance import pdist, squareform\n",
    "    from scipy.cluster.hierarchy import linkage, dendrogram,leaders,fcluster\n",
    "\n",
    "\n",
    "    def KL_H(u,v):\n",
    "        return entropy(u,v)*entropy(v,u)/entropy(u)/entropy(v)\n",
    "\n",
    "\n",
    "    varValues = np.transpose(np.loadtxt(\"./data/allDistComb1Day\"+tit+\".dat\"))\n",
    "    data_dist_a = pdist(varValues,lambda u,v: KL_H(u,v))\n",
    "\n",
    "    varValues2 = (np.loadtxt(\"./data/ldaTopics\"+tit+\".dat\"))\n",
    "    data_dist_b = pdist(varValues2,lambda u,v: KL_H(u,v))\n",
    "\n",
    "    return np.mean(data_dist_a),np.mean(data_dist_b)\n",
    "\n",
    "def find_number_topics(range_lookup,n_iter=1000):\n",
    "    \"\"\"\n",
    "    iterates through range_lookup and plots the distance between topics, to choose the best number\n",
    "    \n",
    "    \"\"\"\n",
    "    allMessages =  pickle.load(open('./data/corpus.dat',\"rb+\"))\n",
    "    vocab = pickle.load(open('./data/vocab.txt',\"rb+\"))\n",
    "        \n",
    "    distances = []\n",
    "    for i in range_lookup:\n",
    "        loglikelihood = lda_run(allMessages,vocab,topicNum = i,n_iter=n_iter)  \n",
    "        freq_distance, topic_distance = find_distances(tit=str(i),retTopics=True)\n",
    "        distances.append([loglikelihood,freq_distance,topic_distance])\n",
    "    \n",
    "    fig = plt.figure(figsize = (15,5))\n",
    "    loglikelihood,freq_distance,topic_distance = zip(*distances)\n",
    "    ax = fig.add_subplot(1,3,1)\n",
    "    plotNumTopics(range_lookup, loglikelihood,ax,loglikelihood=True)\n",
    "    ax = fig.add_subplot(1,3,2)\n",
    "    plotNumTopics(range_lookup, freq_distance,ax,loglikelihood=True)\n",
    "    ax =fig.add_subplot(1,3,3)\n",
    "    plotNumTopics(range_lookup, topic_distance,ax,loglikelihood=True)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "def correlationMerge(tit=\"\",threshold=0.7):\n",
    "    \"\"\"\n",
    "    Cluster topics that are close (in time and in frequency of words)\n",
    "    \"\"\"\n",
    "    from scipy.stats import pearsonr,spearmanr,kendalltau,entropy\n",
    "    import pylab as plt\n",
    "    from scipy.spatial.distance import pdist, squareform\n",
    "    from scipy.cluster.hierarchy import linkage, dendrogram,leaders,fcluster\n",
    "\n",
    "    vocab = pickle.load(open('./data/vocab.txt',\"rb+\"))\n",
    "\n",
    "    def KL_E(u,v):\n",
    "        return entropy(u,v)*entropy(v,u)/entropy(u)/entropy(v)\n",
    "\n",
    "    varValues = np.transpose(np.loadtxt(\"./data/allDistComb1Day\"+str(tit)+\".dat\"))\n",
    "    data_dist_a = pdist(varValues,lambda u,v: KL_E(u,v))\n",
    "\n",
    "    varValues2 = (np.loadtxt(\"./data/ldaTopics\"+str(tit)+\".dat\"))\n",
    "    data_dist_b = pdist(varValues2,lambda u,v: KL_E(u,v))\n",
    "    data_dist = data_dist_a*1 * data_dist_b*1\n",
    "\n",
    "    #print(data_dist)\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "\n",
    "    # plot first dendrogram:\n",
    "    ax1 = fig.add_axes([0.05,0.1,0.2,0.6])\n",
    "    Y = linkage(data_dist, method='weighted') # method?\n",
    "\n",
    "    t = max(Y[:, 2]) * threshold\n",
    "    Z1 = dendrogram(Y, orientation='right',\n",
    "                leaf_font_size=18,color_threshold=t) # adding/removing the axes\n",
    "                #labels=[\"Profit\",\"Decay\",\"Profit \\n(Ratio)\",\"IMDB rating\",\"Metascore\",\"RottenT \\nAudience\",\"RottenT \\nCritics\",\"Profit \\n(Deviation)\"],\n",
    "    ax1.set_xticks([]) # turn off xticks\n",
    "\n",
    "    # plot second dendrogram:\n",
    "    ax2 = fig.add_axes([0.32,0.71,0.58,0.2])\n",
    "    Z2 = dendrogram(Y,color_threshold=t)\n",
    "    ax2.set_xticks([])\n",
    "    ax2.set_yticks([])\n",
    "\n",
    "    clusters = fcluster(Y,t=t,criterion='distance')\n",
    "    #print( clusters)\n",
    "    lisTop = np.asarray(range(len(clusters)))\n",
    "\n",
    "    #print(np.unique(clusters))\n",
    "    prov = 0\n",
    "    for g in np.unique(clusters):\n",
    "        group = lisTop[clusters==g]\n",
    "        for i2 in group:\n",
    "            topic_words = np.array(vocab)[np.argsort(varValues2[i2,:])][:]#-[:n_top_words:-1]\n",
    "            freq_words = np.array(varValues2[i2,:])[np.argsort(varValues2[i2,:])][:]#-n_top_words:-1]\n",
    "            prov += np.sum(freq_words[::-1][:10])\n",
    "\n",
    "\n",
    "    # get the distance matrix:\n",
    "    D = squareform(data_dist)\n",
    "\n",
    "    # reorder rows/cols of D to match dendrograms\n",
    "    idx1 = Z1['leaves']\n",
    "    idx2 = Z2['leaves']\n",
    "    D = D[idx1,:]\n",
    "    D = D[:,idx2]\n",
    "    # add matrix\n",
    "    axmatrix = fig.add_axes([0.32,0.1,0.58,0.6])\n",
    "    im = axmatrix.matshow(D, aspect='auto', origin='lower', cmap=plt.cm.YlGnBu)\n",
    "    axmatrix.set_xticks([])\n",
    "    axmatrix.set_yticks([])\n",
    "\n",
    "    # Plot colorbar.\n",
    "    axcolor = fig.add_axes([0.91,0.1,0.02,0.6])\n",
    "    plt.colorbar(im, cax=axcolor)\n",
    "    #plt.savefig(\"./data/\"+'MatrixPrediction'+tit+'.pdf', bbox_inches='tight' ,dpi=100)\n",
    "    #plt.savefig(\"./data/\"+'MatrixPrediction'+tit+'.png', bbox_inches='tight' ,dpi=100)\n",
    "    plt.show()\n",
    "    return clusters,vocab,varValues2\n",
    "\n",
    "def automatic_annotate(clusters,vocab,varValues2):\n",
    "    \"\"\"\n",
    "    Finds the wods that characterize each cluster (prints them) and by topic (saves them to ./data/topics.csv)\n",
    "    \"\"\"\n",
    "    sumValues = np.sum(varValues2,0)\n",
    "    lisTop = np.asarray(range(len(clusters)))\n",
    "    cols = ['g','r','c','b','m','y','k','g','b']\n",
    "    print(clusters)\n",
    "    \n",
    "    for g in np.unique(clusters):\n",
    "        group = lisTop[clusters==g]\n",
    "        #print(group)\n",
    "        prov = np.zeros(len(sumValues))\n",
    "        for i2 in group:\n",
    "            topic_words = np.array(vocab)[np.argsort(varValues2[i2,:])][:]#-[:n_top_words:-1]\n",
    "            freq_words = np.array(varValues2[i2,:])[np.argsort(varValues2[i2,:])][:]#-n_top_words:-1]\n",
    "            prov += varValues2[i2,:]\n",
    "\n",
    "            sumValues1 = np.sum(varValues2,0)\n",
    "  \n",
    "            topic_words1 = np.array(vocab)[np.argsort(varValues2[i2,:]/sumValues1)][:]#-[:n_top_words:-1]\n",
    "            sumValues2 = np.ones(len(np.sum(varValues2,0)))\n",
    "            topic_words2 = np.array(vocab)[np.argsort(varValues2[i2,:])][:]#-[:n_top_words:-1]           \n",
    "            x = []\n",
    "            indX = 0\n",
    "            while len(x) < 20:\n",
    "                x += list(topic_words1[::-1][indX:indX+1])\n",
    "                x += list(topic_words2[::-1][indX:indX+1])\n",
    "                indX += 1\n",
    "            \n",
    "\n",
    "            print(i2,' '.join(x))\n",
    "            #print('Topic {}: {}'.format(i2, ' '.join(topic_words[::-1][:50])))\n",
    "            #print('Topic {}: {}'.format(i2, ' '.join([str(_) for _ in freq_words[::-1][:50]])))\n",
    "        sumValues1 = np.sum(varValues2,0)\n",
    "        topic_words1 = np.array(vocab)[np.argsort(prov/sumValues)][:]#-[:n_top_words:-1]\n",
    "        sumValues2 = np.ones(len(np.sum(varValues2,0)))\n",
    "        topic_words2 = np.array(vocab)[np.argsort(prov/sumValues)][:]#-[:n_top_words:-1]\n",
    "        x = []\n",
    "        indX = 0\n",
    "        while len(x) < 20:\n",
    "            x += list(topic_words1[::-1][indX:indX+1])\n",
    "            if topic_words2[::-1][indX:indX+1] not in x:\n",
    "                x += list(topic_words2[::-1][indX:indX+1])\n",
    "            indX += 1\n",
    "        \n",
    "        print(\"All cluster: \",' '.join(x))\n",
    "        print(\"-\"*30)\n",
    "   \n",
    "\n",
    "    with open(\"./data/topics.csv\",\"w+\") as f:\n",
    "        for i2 in range(len(clusters)):\n",
    "        \n",
    "            topic_words = np.array(vocab)[np.argsort(varValues2[i2,:])][:]#-[:n_top_words:-1]\n",
    "            freq_words = np.array(varValues2[i2,:])[np.argsort(varValues2[i2,:])][:]#-n_top_words:-1]\n",
    "         \n",
    "            sumValues1 = np.sum(varValues2,0)\n",
    "            topic_words1 = np.array(vocab)[np.argsort(varValues2[i2,:]/sumValues1)][:]#-[:n_top_words:-1]\n",
    "            sumValues2 = np.ones(len(np.sum(varValues2,0)))\n",
    "            topic_words2 = np.array(vocab)[np.argsort(varValues2[i2,:]/sumValues2)][:]#-[:n_top_words:-1]\n",
    "            x = []\n",
    "            indX = 0\n",
    "            while len(x) < 30:\n",
    "                x += list(topic_words1[::-1][indX:indX+1])\n",
    "                x += list(topic_words2[::-1][indX:indX+1])\n",
    "                \n",
    "                indX += 1\n",
    "\n",
    "\n",
    "            f.write(\"- {0}\\t{1}\\n\".format(i2,' '.join(x)))\n",
    "\n",
    "def weightWord(tit=\"\",word='hoi'):\n",
    "    \"\"\"\n",
    "    returns how much weight a word has in every article (based on its topic)\n",
    "    \"\"\"\n",
    "    vocab = pickle.load(open('./data/vocab.txt',\"rb\"))\n",
    "\n",
    "    #By day\n",
    "    varValues = np.transpose(np.loadtxt(\"./data/allDistComb1Day\"+str(tit)+\".dat\"))\n",
    "    #By freq\n",
    "    varValues2 = (np.loadtxt(\"./data/ldaTopics\"+str(tit)+\".dat\"))\n",
    "\n",
    "    weigthTopics = np.ones(int(tit))\n",
    "    \n",
    "    len_vocab = len(vocab)\n",
    "    # Kind-of efficient way to count words (better to use a dictionary)\n",
    "    file = re.split(' |\\n',word)\n",
    "    c = Counter(file)\n",
    "    del c['']\n",
    "    values = np.zeros(len_vocab)\n",
    "    count = 0 \n",
    "    for word in c:\n",
    "        pos = bi_contains(vocab, word)\n",
    "        if pos[0]:\n",
    "            weigthTopics += np.log10(varValues2[:,pos[1]]/np.sum(varValues2[:,pos[1]])*c[word])\n",
    "            \n",
    "\n",
    "    \n",
    "    weigthTopics /= np.sum(weigthTopics)\n",
    "    \n",
    "    return weigthTopics\n",
    "\n",
    "    \n",
    "def plot_imshows(all_topics,filenames,num_topics):\n",
    "    \"\"\"\n",
    "    given all_topics = 2D array, one dimension = topics, the other = different queries we are comparing\n",
    "    \"\"\"\n",
    "    all_topics[all_topics==0] = np.NaN\n",
    "\n",
    "    plt.figure(figsize = (15,10))\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.imshow(all_topics*10,interpolation=\"none\",aspect=\"auto\",cmap=plt.cm.YlGnBu)\n",
    "    plt.yticks(range(len(filenames)),[\"left\",\"right\"])\n",
    "    plt.xticks(range(num_topics),[str(_) for _ in range(num_topics)])\n",
    "    plt.grid(False)\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Not normalized\")\n",
    "    for topic_number in range(num_topics):\n",
    "        all_topics[:,topic_number] = (all_topics[:,topic_number] - np.mean(all_topics[:,topic_number]))/np.std(all_topics[:,topic_number])\n",
    "\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.title(\"Normalized\")\n",
    "    plt.imshow(all_topics,interpolation=\"none\",aspect=\"auto\", cmap=plt.cm.YlGnBu)\n",
    "    plt.yticks(range(len(filenames)),[\"left\",\"right\"])\n",
    "    plt.xticks(range(num_topics),[str(_) for _ in range(num_topics)])\n",
    "    plt.grid(False)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "filenames = [\"./data/Pablo_Iglesias_Podemos.csv\",\"./data/Albert_Rivera_Ciudadanos.csv\",\"./data/Mariano_Rajoy_PP.csv\",\"./data/Pedro_Sanchez_PSOE.csv\"]\n",
    "separator_csv = \"\\t\" #tab\n",
    "language = \"spanish\"\n",
    "cached_stop=stopwords.words(language)\n",
    "## Create dictionary with words \n",
    "create_dictionary(lang=language, cached_stop=stopwords.words(language))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "left_messages_groupped = [\" \".join(left_messages[i:i+100]) for i in range(0,len(left_messages)-100,100)]\n",
    "right_messages_groupped = [\" \".join(right_messages[i:i+100]) for i in range(0,len(right_messages)-100,100)]\n",
    "## Create corpus (counts words for every article)\n",
    "create_corpus(left_messages_groupped+right_messages_groupped[:len(left_messages_groupped)],language=language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "find_number_topics(range(10,40,2),n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "find_number_topics([25],n_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cluster topics\n",
    "clusters,vocab,varValues2 = correlationMerge(25,threshold=0.4)\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#annotate\n",
    "automatic_annotate(clusters,vocab,varValues2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Plots what topics are important for the different data sets\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "num_topics = 25\n",
    "filenames = [left_messages_groupped,right_messages_groupped]\n",
    "all_topics = np.zeros((len(filenames),num_topics))\n",
    "## Fill up\n",
    "for i,filename in enumerate(filenames):\n",
    "    all_messages =  \" \".join(filename)\n",
    "    mess = remove_stop_words(remove_punctuation(all_messages),cached_stop)\n",
    "    all_topics[i,:] = weightWord(num_topics,word=mess)\n",
    "    print(all_topics[i,:])\n",
    "\n",
    "plot_imshows(all_topics,filenames,num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.utils.extmath import density\n",
    "from pprint import pprint\n",
    "def classifier(clf,X_train,y_train,X_test,feature_names,categories):\n",
    "    # Adapted from below\n",
    "    # Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n",
    "    #         Olivier Grisel <olivier.grisel@ensta.org>\n",
    "    #         Mathieu Blondel <mathieu@mblondel.org>\n",
    "    #         Lars Buitinck <L.J.Buitinck@uva.nl>\n",
    "    # License: BSD 3 clause\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "    pred_prob = clf.predict_proba(X_test)\n",
    "    print(clf.coef_)\n",
    "\n",
    "    print(\"top 10 keywords per class:\")\n",
    "    for i, category in enumerate(categories):\n",
    "        top10 = np.argsort(clf.coef_[i])[-30:]\n",
    "        print(\"{0}: {1}\".format(category, \" \".join(feature_names[top10])))\n",
    "    print()\n",
    "\n",
    "\n",
    "    return pred,pred_prob\n",
    "\n",
    "\n",
    "\n",
    "print('data loaded')\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words=cached_stop)\n",
    "categories = [\"podemos\",\"pp\"]\n",
    "llm = len(left_messages_groupped)\n",
    "lrm = len(right_messages_groupped)\n",
    "data_train = left_messages_groupped[:int(llm/2)] + right_messages_groupped[:int(lrm/2)] + [\"a\"]\n",
    "data_test = left_messages_groupped[int(llm/2):] + right_messages_groupped[int(lrm/2):] + [\"a\"]\n",
    "y_train = [1]*int(llm/2)+[2]*int(lrm/2)+[3]\n",
    "y_test = [1]*(llm-int(llm/2))+[2]*(lrm-int(lrm/2))+[3]\n",
    "\n",
    "\n",
    "X_train = vectorizer.fit_transform(data_train)\n",
    "X_test = vectorizer.transform(data_test)\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "pred,pred_prob = classifier(MultinomialNB(alpha=.1),X_train,y_train,X_test,feature_names,categories)\n",
    "np.sum(pred==np.array(y_test))/len(pred),np.sum(np.array(y_test)==2)/len(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
